{
  "metadata": {
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-autonumbering": true,
    "toc-showtags": false,
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Importing Libraries",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# 1. Operational\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\n\n# 2. Ploting\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick # to format y axis as a percentage instead of decimal value\nimport matplotlib as mpl # to format fonts\nimport seaborn as sns             # ploting (easier and focused on stats)\nimport plotly.express as px       # ploting beautiful and interactive\n\n# 3. Statistics\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols    # to create regression models\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n# 4. Machine Learning\n\n# Important imports for preprocessing, modeling, and evaluation.\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.metrics as metrics\nfrom sklearn.preprocessing import StandardScaler # to normalize variables\nfrom sklearn.naive_bayes import GaussianNB       # to construct a Naive Bayes model\n\n# Decision trees\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree               # This function displays the splits of the tree\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Boosting\nfrom xgboost import XGBClassifier                # classifier\nfrom xgboost import plot_importance              # plot feature importance \n\n# Save models once we fit them\nimport pickle",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'seaborn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmtick\u001b[39;00m \u001b[38;5;66;03m# to format y axis as a percentage instead of decimal value\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m \u001b[38;5;66;03m# to format fonts\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m             \u001b[38;5;66;03m# ploting (easier and focused on stats)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m       \u001b[38;5;66;03m# ploting beautiful and interactive\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 3. Statistics\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "!pip install --upgrade pexpect\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "ename": "<class 'AttributeError'>",
          "evalue": "module 'pexpect' has no attribute 'TIMEOUT'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install --upgrade pexpect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2590\u001b[0m, in \u001b[0;36mInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2587\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m   2588\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[0;32m-> 2590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/utils/_process_posix.py:129\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    125\u001b[0m enc \u001b[38;5;241m=\u001b[39m DEFAULT_ENCODING\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Patterns to match on the output, for pexpect.  We read input and\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# allow either a short timeout or EOF\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m patterns \u001b[38;5;241m=\u001b[39m [\u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m, pexpect\u001b[38;5;241m.\u001b[39mEOF]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# the index of the EOF pattern in the list.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# even though we know it's 1, this call means we don't have to worry if\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# we change the above list, and forget to change this value:\u001b[39;00m\n\u001b[1;32m    133\u001b[0m EOF_index \u001b[38;5;241m=\u001b[39m patterns\u001b[38;5;241m.\u001b[39mindex(pexpect\u001b[38;5;241m.\u001b[39mEOF)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pexpect' has no attribute 'TIMEOUT'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Importing data",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## From .csv",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Using pandas\ndf = pd.read_csv(#input filepath here)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# using csv module\nwith open(“#filename / file path”, #'mode’)\n    csv_df = file.read()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Modes:\n- ‘r’ – read\n- ‘w’ – write\n- ‘a’ – append\n- ‘+’ – create new file",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## From Excel",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_excel(file_path\\file_name.xlsx, \n                    sheet_name=name_of_your_sheet,\n                    skiprows=number_of_rows_to_skip,\n                    usecols=\"A:D\")\n\n# It might be necessary to use “\\\\” instead of just “\\”.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## From SQL",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Big Query\n\n# Use IPython magic commands to connect to BigQuery.\n%load_ext google.cloud.bigquery\n\n# Input the following magic command “%%bigquery” \n# along with the name of any set of data in the database.\n%%bigquery – country_names_area df\n\n# Use SQL commands to select the data you want to use.\nSELECT * FROM `country_names_area`",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Get to know your data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv('Python KB/sales_data.csv')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "data = {\"user\" : [1,2,3,4,5,6,7,8,9,10],\n        \"date\" : [\"2020-08-10\",\"2020-02-23\", \"2020-06-17\", \"2021-03-07\", \"2021-12-30\", \"2021-05-14\", \"2021-09-26\", \"2022-02-08\" ,\"2022-02-14\" ,\"2022-04-26\"],\n        \"country\" : [\"Portugal\", \"Spain\", \"Italy\", \"France\", \"Germany\", \"Grece\", \"Portugal\", \"Spain\", \"Italy\", \"France\"],\n        \"number\" : [12, 15, 40, 30, 25, 500, 20, 26, 34, 26]}\n          \ndf = pd.DataFrame(data)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Returns the number os rows and columns in a tupple(?)\ndf.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Returns a list of column names\ndf.columns",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.info()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.describe()\ndf.describe(include = \"all\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df[\"column\"].value_counts()\ndf[\"column\"].value_counts(normalize=True)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Data types",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.dtypes",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "type(variable)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "variable_1 = \"python\"\nvariable_2 = 200\n\nprint(type(variable_1))\nprint(type(variable_2))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Clean data",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Duplicates",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This function returns a series of “true/false” outputs, indicating if a data value is duplicate or unique.\npd.duplicated()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Looking for duplicates only in the column \"brand\"\n# and keeping the last value, ie, every instance of that value before it will be considered the duplicate.\ndf.duplicated(subset=[\"brand\"], keep=\"last\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# This function will help create a new dataframe with all of the duplicate rows removed.\ndf2 = df.drop_duplicates()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Missing data",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Find missing values",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Find the number of missing values in each column in this dataset.\ndf.isnull().sum()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Filter the DataFrame to only include rows with at least one missing value.\ndf_missing = df[ df.isna().any(axis=1) ]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Create a new df of just the rows that are missing data on the state_code column\ndf_null = df[pd.isnull(df.state_code)]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Remove missing values",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df2 = df.dropna()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Remove all rows with missing data:\ndf.dropna(inplace = True)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Remove rows with missing values in the \"Sales\" column:\ndf = df.dropna(subset = [\"Sales\"], axis = 0)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Outliers",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Calculate 25th percentile\npercentile25 = df['columns'].quantile(0.25)\n\n# Calculate 75th percentile\npercentile75 = df['columns'].quantile(0.75)\n\n# Calculate interquartile range\niqr = percentile75 - percentile25\n\n# Calculate upper and lower thresholds for outliers\nupper_limit = percentile75 + 1.5 * iqr\nlower_limit = percentile25 - 1.5 * iqr",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Isolate outliers on low end\ndf[df['column'] < lower_limit]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Remove outliers",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create new df that removes outliers\ndf_without_outliers = df[df['column'] >= lower_limit]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ... or use a mask\nmask = (df['column'] >= lower_limit) & (df['column'] <= upper_limit)\ndf = df[mask].copy()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Reassign outliers",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Calculate 10th percentile\ntenth_percentile = np.percentile(df['column'], 10)\n\n# Calculate 90th percentile\nninetieth_percentile = np.percentile(df['column'], 90)\n\n# Apply lambda function to replace outliers with thresholds defined above\ndf['column'] = df['column'].apply(lambda x: (\n    tenth_percentile if x < tenth_percentile \n    else ninetieth_percentile if x > ninetieth_percentile \n    else x))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Imput the average",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Calculate 10th percentile\ntenth_percentile = np.percentile(df['column'], 10)\n\n# Calculate 90th percentile\nninetieth_percentile = np.percentile(df['column'], 90)\n\n# Apply lambda function to replace outliers with thresholds defined above\ndf['column'] = df['column'].apply(lambda x: (\n    tenth_percentile if x < tenth_percentile \n    else ninetieth_percentile if x > ninetieth_percentile \n    else x))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Logical ranges",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Validate date column to make sure the dates present in the dataframe are inside the expected date ranges \n\n# Create datetime index of every date in 2018\nfull_date_range = pd.date_range(start='2018-01-01', end='2018-12-31')\n\n# Determine which values are in `full_date_range` but not in `df['date']`\nfull_date_range.difference(df['date'])\n\nResult:\n\nDatetimeIndex(['2018-06-19', '2018-06-20', '2018-06-21', '2018-06-22',\n               '2018-09-18', '2018-09-19', '2018-12-01', '2018-12-02'],\n              dtype='datetime64[ns]', freq=None)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Drop columns",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Change columns names",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.columns = [\"column_one\", \"column_two\", \"column_three\"]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df = df.rename(columns={'column_1': 'column_one'})",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Change date types",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = df.astype({\"Year\": int,\n                \"Total\": int,\n                \"Male\": int,\n                \"Female\": int})",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Categorical data and numerical data",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Order categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create categorical designations\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\n# Encode `month` column as categoricals \ndf['month'] = pd.Categorical(df['month'], categories=months, ordered=True)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Create bucket categories for quantities",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create a new column that categorizes number_of_strikes into 1 of 4 categories\ndf_by_month['strike_level'] = pd.qcut(\n    df_by_month['number_of_strikes'],\n    4,\n    labels = ['Mild', 'Scattered', 'Heavy', 'Severe'])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Assign number to categories",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create new column representing numerical value of strike level\ndf_by_month['strike_level_code'] = df_by_month['strike_level'].cat.codes",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Get dummies",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_dummies = pd.get_dummies(df_by_month['strike_level'])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df_dummies = pd.get_dummies(df_by_month['strike_level'])# Use the pd.concat function to join the dummy columns \n#  to the original dataframe.\n\ndf_by_month = pd.concat([df_by_month, df_dummies], axis = 1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### LabelEncoder()",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Datetime manipulation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Convert date column to datetime\ndf['date']= pd.to_datetime(df['date'])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Month",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get month number\ndf['month'] = df['date'].dt.month\n# Get month name\ndf['month_name'] = df['date'].dt.month_name()\n# Get month name with only the first 3 letters\ndf['month_txt'] = df['date'].dt.month_name().str.slice(stop=3)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Week",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get number of the week in the year\ndf['week'] = df.date.dt.isocalendar().week\n# Get name of the day of the week\ndf['weekday'] = df[\"date\"].dt.day_name()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Create duration in minutes between two dates.\ndf['time_difference'] = (df['end_time'] - df['start_time']).dt.total_seconds() / 60",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Strings",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "variable_2 = variable_1.swapcase()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Structuring data\nIn this section: \n- Sorting\n- Filtering\n- Extracting\n- Slicing\n- Grouping\n- Merging\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Sorting\nSorting is the process of arranging data into meaningful order.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.sort_values(by=['column'], ascending = True, inplace = False)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Sorting gradient\n\n# Identify top 20 locations with most days of stikes\ndf.center_point_geom.value_counts()[:16].\n    rename_axis('unique_values').\n    reset_index(name='counts').\n    style.background_gradient()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Filtering\nFiltering is the process of selecting a smaller part of your dataset based on specified parameters, you can think of filtering like selecting rows of a dataset.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df[df['country']=='Portugal']",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Filtering based on a list\ndf[ df[\"country\"].isin([\"Portugal\", \"Spain\", \"Italy\"]) ]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# AND\ndf[(df['column_1'] > 60) & (df['column_2'] == 3)]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# OR\ndf[(df['column_1'] > 60) | (df['column_2'] == 3)]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Boolean masking\nBoolean masking is a filtering technique that overlays a Boolean grid onto a dataframe in order to select only the values in the dataframe that align with the True values of the grid.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Extracting\nExtracting is the process of retrieving data from a dataset or source for further processing, you can think of extraction as retrieving whole columns of data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df[[\"column_1\", \"columns_2\"]]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Returns columns based on the column dtypes (e.g., float64, int64, bool, object, etc.).\ndf2 = df.select_dtypes(include=['int64'])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Slicing\nSlicing breaks information down into smaller parts to facilitate efficient examination and analysis from different viewpoints. Think of slicing as an either or both options for columns and rows. A combination of extraction and filtering.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# df.iloc[] - to slice a dataframe based on an integer index location.\n\ndf.iloc[5:10, 2:]           # selects only rows 5 through 9, at columns 2+ \ndf.iloc[5:10]               # selects only rows 5 through 9, all columns\ndf.iloc[1, 2]               # selects value at row 1, column 2\ndf.iloc[[0, 2], [2, 4]]     # selects only rows 0 and 2, at columns 2 and 4\n\n# df.loc[] - to slice a dataframe based on a label or Boolean array.\n\ndf.loc[:, ['column_1', 'column_2']]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Grouping\nGrouping sometimes called bucketizing, is aggregating individual observations of a variable into groups. It is like creating categories for numerical values based on ranges.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Calculate mean count of strikes for each weekday\ndf[['weekday','number_of_strikes']].groupby(['weekday']).mean()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# grouping is often accompained by aggregations\n\n# Calculate total strikes for each month of each year\ncount_by_month = df.groupby(['month','year']).agg(\n    number_of_strikes = pd.NamedAgg(column='number_of_strikes',aggfunc=sum)\n    ).reset_index()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Merging\nMerging is a method to combine two different data frames along a specified starting column.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Merge\ndf1.merge(df2, \n            how=‘inner’, \n            on=[‘month’,’year’])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Concat\ndf3 = pd.concat([df1.drop( ['column_1','column_2'], axis=1 ), df2])\n\n# In this example, df2 does not have the columns 'column_1' and 'column_2'.\n# We drop those columns to avoid errors.\n# axis = 1 tells the computer that 'column_1' and 'column_2' are columns.\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Join\ndf1.set_index('key').join(df2.set_index('key'))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Make calculations",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Summary statistics\ndf[\"column\"].max()\ndf[\"column\"].min()\ndf[\"column\"].mean()\ndf[\"column\"].mstd()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\ndf[\"column\"].value_counts()\ndf[\"column\"].value_counts(normalize=True)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(\"Minumun percentage of females: \", \"{0:.0%}\".format(min(df_gender[\"Female Percentage\"])))\nprint(\"Maximum percentage of females: \", \"{0:.0%}\".format(max(df_gender[\"Female Percentage\"])))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Plotting",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Graph basics",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.title('Old Faithful geyser - time between eruptions')\nplt.xlabel(\"Months(2018)\")\nplt.ylabel(\"Number of lightning strikes\")\n\nplt.legend()\n# change location of legend\nplt.legend(frameon = False, \n    loc = \"upper left\", \n    ncol = 3) # number of columns can be used for an horizontal legend\n\n# remove frame around graph\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# remove frame around legend\nplt.legend(frameon = False)\n\n\nax.set_xticks(range(35, 101, 5))\nax.set_yticks(range(0, 61, 10))\n\n\n# Change figure lenght to 20 and height to 10 \nplt.rcParams[\"figure.figsize\"] = (20,10)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Formatting axis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# set the range for the y axis\nplt.ylim(0,) \n# or\nplt.ylim(0,1000)\n\n# format y axis as thousands\nplt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: '{:,.0f}K'.format(x/1000)))\nplt.tick_params(axis='y', which='major', labelsize=10)\n\n# format y axis as percentage\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=None, symbol='%', is_latex=False))\n\n# Rotate the X-Axis Ticks by 45-degrees\nplt.xticks(rotation = 45)    \n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "plt.gca() is used to call the current plot.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Display 2 visuals side to side",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create a 1x2 plot figures.\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Create a histogram with the residuals. \nsns.histplot(residuals, ax=axes[0])\naxes[0].set_xlabel(\"Residual Value\")\naxes[0].set_title(\"Histogram of Residuals\")\n\n# Create a Q-Q plot of the residuals.\nsm.qqplot(residuals, line = \"s\", ax=axes[1])\naxes[1].set_title(\"Q-Q plot of Residuals\")\n\n# Use matplotlib's tight_layout() function to add space between plots for a cleaner appearance.\nplt.tight_layout()\n\n# Show the plot.\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Annotations",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# add a vertical line in 2007 \nplt.axvline(x = 2007, color = 'silver', linestyle = \"--\") \nplt.annotate(\"2007\",\n            xy = (2008, 750000)) # position of note",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Function to plot labels",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "def addlabels(x, y, labels):\n    '''\n    Iterates over data and plots text labels above each bar of bar graph.\n    '''\n    for i in range(len(x)):\n        plt.text(i, y[i], labels[i], ha = 'center', va = 'bottom')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Save graph as png",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.savefig(\"C:\\\\Users\\\\raque\\\\Documents\\\\Data Analytics\\\\Portuguese Agriculture Python\\\\output graphs\\\\gender.png\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Bar graph",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.bar(x = df_by_month['month'],\n    height = df_by_month['number_of_strikes'], \n    label = \"Number of strikes\")\nplt.plot()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Note that here we use sns to create the visual\np = sns.barplot(\n    data = df_by_quarter,\n    x = 'quarter_number',\n    y = 'number_of_strikes',\n    hue = 'year')\n\n# Annotating data labels\nfor b in p.patches:\n    p.annotate(\n\t\t\tstr(round(b.get_height()/1000000, 1))+'M',                   # Text\n      (b.get_x() + b.get_width() / 2., b.get_height() + 1.2e6),    # Position \n      ha = 'center', va = 'bottom', \n      xytext = (0, -12), \n      textcoords = 'offset points')\n\n# Adjust aestethics\nplt.xlabel(\"Quarter\")\nplt.ylabel(\"Number of lightning strikes\")\nplt.title(\"Number of lightning strikes per quarter (2016-2018)\")\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Line graph",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.lineplot(x = df[\"column_1\"], y = df[\"column_2\"])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Plotting a time series with multiple series:\n\nplt.plot(df_gender[\"Year\"], \n         df_gender[\"Male\"], \n         label = \"Males\",      # not mandatory\n         color = \"#5B9BD5\",    # not mandatory\n         linestyle = \"--\")     # not mandatory\nplt.plot(df_gender[\"Year\"],\n         df_gender[\"Female\"], \n         label = \"Females\", \n         color = \"#ED7D31\")\n\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Box plot",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "g = sns.boxplot(\n    data=df, \n    x='weekday',\n    y='number_of_strikes', \n    order=['Monday','Tuesday', 'Wednesday', 'Thursday','Friday','Saturday','Sunday'], \n    showfliers=False \n    );",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Histogram",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ax = sns.histplot(\n    df['seconds'], \n    binrange=(40, 100), \n    binwidth=5, \n    color='#4285F4', \n    alpha=1)               # transparancy \n\nax.set_xticks(range(35, 101, 5))\nax.set_yticks(range(0, 61, 10))\n\nplt.show();",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Scatterplot",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.scatterplot(x = df['column_1'], y = df['column_2'])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Heatmap",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.heatmap(df, annot = True, cmap = \"Reds\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Pairplot",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.pairplot(df)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Statistics",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Logics / algorithms / functions",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Defining classes with unique attributes and methods",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## While loop",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## For loop",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Sample dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get a sample from the dataframe\nsample_companies = companies.sample(n = 50, random_state = 1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}